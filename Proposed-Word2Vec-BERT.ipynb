{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Word2Vec-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForSequenceClassification, logging\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gensim.downloader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import datasets\n",
    "import string\n",
    "import evaluate\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "logging.set_verbosity_error()\n",
    "logging.set_verbosity_warning()\n",
    "HF_HUB_DISABLE_SYMLINKS_WARNING = True\n",
    "\n",
    "import datetime\n",
    "t_delta = datetime.timedelta(hours=9)\n",
    "JST = datetime.timezone(t_delta, 'JST')\n",
    "now = datetime.datetime.now(JST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "MODEL = \"bert-base-uncased\"\n",
    "# X_TRAIN = '../dataset/Proposed-Word2Vec-BERT_x_train.npy'\n",
    "# Y_TRAIN = '../dataset/Proposed-Word2Vec-BERT_y_train.npy'\n",
    "# X_TEST = '../dataset/Proposed-Word2Vec-BERT_x_test.npy'\n",
    "# Y_TEST = '../dataset/Proposed-Word2Vec-BERT_y_test.npy'\n",
    "SAVED_MODEL = \"../model/Proposed-Word2Vec-BERT_\"+str(now.strftime('%Y%m%d%H%M%S'))\n",
    "THRESHOLD = 0.05\n",
    "MAXLEN_GET_PSEUDO = 3000\n",
    "EPOCH = 5\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../model/Proposed-Word2Vec-BERT_20221121011706\n"
     ]
    }
   ],
   "source": [
    "print(SAVED_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理\n",
    "def preprocessing(text):\n",
    "    # 括弧内文章の削除\n",
    "    text = re.sub(r'\\(.*\\)',' ',text)\n",
    "    text = re.sub(r'\\[.*\\]',' ',text)\n",
    "    text = re.sub(r'\\<.*\\>',' ',text)\n",
    "    text = re.sub(r'\\{.*\\}',' ',text)\n",
    "    # 記号文字の削除\n",
    "    text = text.translate(str.maketrans('','',string.punctuation))\n",
    "    # スペースの調整\n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18846/18846 [00:01<00:00, 17907.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# 20 newsgroups datasets\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups = fetch_20newsgroups(subset=\"all\")\n",
    "newsgroups_datasets = list()\n",
    "\n",
    "# # example ------------------------------------------------\n",
    "# for texts in tqdm(newsgroups.data[:10000]):\n",
    "#   texts = texts.split(\"\\n\\n\")\n",
    "#   texts = \" \".join(texts[1:])\n",
    "#   newsgroups_datasets.append(preprocessing(texts))\n",
    "# # --------------------------------------------------------\n",
    "\n",
    "for texts in tqdm(newsgroups.data):\n",
    "  texts = texts.split(\"\\n\\n\")\n",
    "  texts = \" \".join(texts[1:])\n",
    "  newsgroups_datasets.append(preprocessing(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1300000/1300000 [00:28<00:00, 45614.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# yahoo topic datasets\n",
    "with open('../data/topic/train_pu_half_v0.txt','r',encoding='utf-8') as f:\n",
    "    texts_v0 = f.read()\n",
    "with open('../data/topic/train_pu_half_v1.txt','r',encoding='utf-8') as f:\n",
    "    texts_v1 = f.read()\n",
    "texts = texts_v0 + texts_v1\n",
    "topic_datasets = list()\n",
    "\n",
    "# # example ----------------------------------------------\n",
    "# for label_text in tqdm(texts.splitlines()[:10000]):\n",
    "#   _, text = label_text.split(\"\\t\")\n",
    "#   topic_datasets.append(preprocessing(text))\n",
    "# # -------------------------------------------------------\n",
    "\n",
    "for label_text in tqdm(texts.splitlines()):\n",
    "  _, text = label_text.split(\"\\t\")\n",
    "  topic_datasets.append(preprocessing(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 762027/762027 [00:31<00:00, 24469.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# reuters datasets\n",
    "with open(\"../data/reuter/sourceall.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "  reuter = f.read().split(\"\\n\")[:-1]\n",
    "\n",
    "# # example -----------------------------------\n",
    "# reuter = reuter[:10000]\n",
    "# # -------------------------------------------\n",
    "\n",
    "reuters_datasets = list()\n",
    "for label_text in tqdm(reuter):\n",
    "  _, text = label_text.split(\"\\t\")\n",
    "  reuters_datasets.append(preprocessing(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 560000/560000 [00:08<00:00, 65346.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# dbpedia datasets train\n",
    "with open('../data/dbpedia_csv/train.csv','r',encoding='utf-8') as f:\n",
    "    reader = [r for r in csv.reader(f)]\n",
    "    \n",
    "# # example -------------------\n",
    "# reader = reader[:10000]\n",
    "# #----------------------------\n",
    "\n",
    "dbpedia_train_datasets = list()\n",
    "for _, auth, text in tqdm(reader):\n",
    "    text = text.replace(auth,'')\n",
    "    dbpedia_train_datasets.append(preprocessing(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbpedia classes\n",
    "with open(\"../data/dbpedia_csv/classes.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "  classes = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_texts = newsgroups_datasets + topic_datasets + reuters_datasets + dbpedia_train_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choice method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = gensim.downloader.load('word2vec-google-news-300')\n",
    "\n",
    "def w2v_avg_vector(sentence):\n",
    "  vector = np.zeros((300,), dtype=\"float32\")\n",
    "  count = 0\n",
    "  for word in sentence.split():\n",
    "    try:\n",
    "      vector = np.add(vector, word2vec[word])\n",
    "      count += 1\n",
    "    except:\n",
    "      pass\n",
    "  if count > 0:\n",
    "    vector = np.divide(vector, len(word))\n",
    "  return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_vector = list()\n",
    "for cls in classes:\n",
    "  classes_vector.append(w2v_avg_vector(cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2640873/2640873 [12:44<00:00, 3455.20it/s]\n"
     ]
    }
   ],
   "source": [
    "diff_datasets = {i:[] for i in range(len(classes))}\n",
    "for texts in tqdm(datasets_texts):\n",
    "  texts_vector = w2v_avg_vector(texts)\n",
    "  similarity = cosine_similarity([texts_vector], classes_vector)[0]\n",
    "  sim_argsorted = np.argsort(similarity)\n",
    "  diff = similarity[sim_argsorted[-1]] - similarity[sim_argsorted[-2]]\n",
    "  if diff > THRESHOLD:\n",
    "    diff_datasets[sim_argsorted[-1]].append((similarity[sim_argsorted[-1]], texts))\n",
    "\n",
    "pseudo_texts = list()\n",
    "pseudo_labels = list()\n",
    "for i in range(len(classes)):\n",
    "  sorted_diff_data = sorted(diff_datasets[i], reverse=True)[:MAXLEN_GET_PSEUDO]\n",
    "  pseudo_texts.extend([i[1] for i in sorted_diff_data])\n",
    "  pseudo_labels.extend([i]*len(sorted_diff_data[:MAXLEN_GET_PSEUDO]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all selected data\n",
      "Com. : 61181\n",
      "Edu. : 34672\n",
      "Art. : 6818\n",
      "Ath. : 16123\n",
      "Off. : 126858\n",
      "Mea. : 144368\n",
      "Bui. : 10120\n",
      "Nat. : 38562\n",
      "Vil. : 35431\n",
      "Ani. : 4742\n",
      "Pla. : 13933\n",
      "Alb. : 42379\n",
      "Fil. : 26335\n",
      "Wri. : 23263\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of all selected data\")\n",
    "for i in diff_datasets:\n",
    "  print(classes[i][:3]+\". : \"+str(len(diff_datasets[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:01<00:00, 66331.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# load test data\n",
    "# dbpedia datasets train\n",
    "with open('../data/dbpedia_csv/test.csv','r',encoding='utf-8') as f:\n",
    "    reader = [r for r in csv.reader(f)]\n",
    "    \n",
    "# # example -------------------\n",
    "# import random\n",
    "# reader = random.sample(reader, 10000)\n",
    "# #----------------------------\n",
    "\n",
    "test_texts = list()\n",
    "test_labels = list()\n",
    "for labels, auth, text in tqdm(reader):\n",
    "    text = text.replace(auth,'')\n",
    "    test_texts.append(preprocessing(text))\n",
    "    test_labels.append(int(labels)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f675677030a4a269c39b0886465d703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d003a39c976143c4a36d5f36010073d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 42000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 70000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "train_dataset = datasets.Dataset.from_dict({\"text\":pseudo_texts, \"label\":pseudo_labels})\n",
    "test_dataset = datasets.Dataset.from_dict({\"text\":test_texts, \"label\":test_labels})\n",
    "dataset = datasets.DatasetDict({\"train\":train_dataset, \"test\":test_dataset})\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, return_tensors=\"pt\", padding=\"max_length\", max_length=512)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns('text')\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42) #.select(range(5000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42) #.select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return evaluate.load(\"accuracy\").compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "  output_dir=SAVED_MODEL,\n",
    "  num_train_epochs=EPOCH,\n",
    "  per_device_train_batch_size=BATCH_SIZE,\n",
    "  per_device_eval_batch_size=BATCH_SIZE,\n",
    "  evaluation_strategy=\"epoch\",\n",
    "  optim=\"adamw_torch\",\n",
    "  report_to=\"none\"\n",
    "  )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 42000\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 13125\n",
      "  Number of trainable parameters = 109493006\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f55945f46b2749ad933157c3ca10b291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-500\n",
      "Configuration saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4291, 'learning_rate': 4.80952380952381e-05, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-1000\n",
      "Configuration saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-1000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1498, 'learning_rate': 4.6190476190476194e-05, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-1500\n",
      "Configuration saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-1500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.138, 'learning_rate': 4.428571428571428e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-2000\n",
      "Configuration saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-2000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1185, 'learning_rate': 4.2380952380952385e-05, 'epoch': 0.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-2500\n",
      "Configuration saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-2500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1002, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-2500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 70000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96178ea9ee5a4649839f6b4b2eb083fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.14278244972229, 'eval_accuracy': 0.6490857142857143, 'eval_runtime': 551.4805, 'eval_samples_per_second': 126.931, 'eval_steps_per_second': 7.933, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-3000\n",
      "Configuration saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-3000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0538, 'learning_rate': 3.857142857142858e-05, 'epoch': 1.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-3500\n",
      "Configuration saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-3500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0615, 'learning_rate': 3.6666666666666666e-05, 'epoch': 1.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-3500/pytorch_model.bin\n",
      "Saving model checkpoint to ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-4000\n",
      "Configuration saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-4000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0497, 'learning_rate': 3.476190476190476e-05, 'epoch': 1.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-4500\n",
      "Configuration saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-4500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0492, 'learning_rate': 3.285714285714286e-05, 'epoch': 1.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-5000\n",
      "Configuration saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-5000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0584, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-5000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 70000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a2c1d26fd2475fb26817d57219550f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.3421435356140137, 'eval_accuracy': 0.6529571428571429, 'eval_runtime': 552.021, 'eval_samples_per_second': 126.807, 'eval_steps_per_second': 7.925, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-5500\n",
      "Configuration saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-5500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0423, 'learning_rate': 2.9047619047619052e-05, 'epoch': 2.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-5500/pytorch_model.bin\n",
      "Saving model checkpoint to ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-6000\n",
      "Configuration saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-6000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0284, 'learning_rate': 2.714285714285714e-05, 'epoch': 2.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-6000/pytorch_model.bin\n",
      "Saving model checkpoint to ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-6500\n",
      "Configuration saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-6500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0241, 'learning_rate': 2.523809523809524e-05, 'epoch': 2.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-6500/pytorch_model.bin\n",
      "Saving model checkpoint to ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-7000\n",
      "Configuration saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-7000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.016, 'learning_rate': 2.3333333333333336e-05, 'epoch': 2.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-7000/pytorch_model.bin\n",
      "Saving model checkpoint to ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-7500\n",
      "Configuration saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-7500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0208, 'learning_rate': 2.1428571428571428e-05, 'epoch': 2.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-7500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 70000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "363a1d2e13404562bb16e98405547853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.9975690841674805, 'eval_accuracy': 0.6280571428571429, 'eval_runtime': 551.1018, 'eval_samples_per_second': 127.018, 'eval_steps_per_second': 7.939, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-8000\n",
      "Configuration saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-8000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0188, 'learning_rate': 1.9523809523809524e-05, 'epoch': 3.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-8000/pytorch_model.bin\n",
      "Saving model checkpoint to ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-8500\n",
      "Configuration saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-8500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0056, 'learning_rate': 1.761904761904762e-05, 'epoch': 3.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-8500/pytorch_model.bin\n",
      "Saving model checkpoint to ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-9000\n",
      "Configuration saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-9000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0132, 'learning_rate': 1.5714285714285715e-05, 'epoch': 3.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-9000/pytorch_model.bin\n",
      "Saving model checkpoint to ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-9500\n",
      "Configuration saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-9500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0132, 'learning_rate': 1.3809523809523811e-05, 'epoch': 3.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-9500/pytorch_model.bin\n",
      "Saving model checkpoint to ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-10000\n",
      "Configuration saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-10000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0032, 'learning_rate': 1.1904761904761905e-05, 'epoch': 3.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-10000/pytorch_model.bin\n",
      "Saving model checkpoint to ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-10500\n",
      "Configuration saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-10500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0085, 'learning_rate': 1e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-10500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 70000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b53c54db844487590ccd17cf8681c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.5518910884857178, 'eval_accuracy': 0.5980857142857143, 'eval_runtime': 551.1457, 'eval_samples_per_second': 127.008, 'eval_steps_per_second': 7.938, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-11000\n",
      "Configuration saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-11000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0011, 'learning_rate': 8.095238095238097e-06, 'epoch': 4.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-11000/pytorch_model.bin\n",
      "Saving model checkpoint to ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-11500\n",
      "Configuration saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-11500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0014, 'learning_rate': 6.190476190476191e-06, 'epoch': 4.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-11500/pytorch_model.bin\n",
      "Saving model checkpoint to ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-12000\n",
      "Configuration saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-12000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0033, 'learning_rate': 4.285714285714286e-06, 'epoch': 4.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-12000/pytorch_model.bin\n",
      "Saving model checkpoint to ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-12500\n",
      "Configuration saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-12500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0012, 'learning_rate': 2.3809523809523808e-06, 'epoch': 4.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-12500/pytorch_model.bin\n",
      "Saving model checkpoint to ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-13000\n",
      "Configuration saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-13000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0021, 'learning_rate': 4.761904761904763e-07, 'epoch': 4.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../model/Proposed-Word2Vec-BERT_20221121011706/checkpoint-13000/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 70000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d640e2c70c4f809ecdd76d23acca21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.3852851390838623, 'eval_accuracy': 0.6295428571428572, 'eval_runtime': 551.5755, 'eval_samples_per_second': 126.909, 'eval_steps_per_second': 7.932, 'epoch': 5.0}\n",
      "{'train_runtime': 7557.7703, 'train_samples_per_second': 27.786, 'train_steps_per_second': 1.737, 'train_loss': 0.05376812645292708, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=13125, training_loss=0.05376812645292708, metrics={'train_runtime': 7557.7703, 'train_samples_per_second': 27.786, 'train_steps_per_second': 1.737, 'train_loss': 0.05376812645292708, 'epoch': 5.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../model/Proposed-Word2Vec-BERT_20221121011706/config.json\n",
      "Model weights saved in ../model/Proposed-Word2Vec-BERT_20221121011706/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(SAVED_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../model/Proposed-Word2Vec-BERT_20221121011706/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"../model/Proposed-Word2Vec-BERT_20221121011706\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file ../model/Proposed-Word2Vec-BERT_20221121011706/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ../model/Proposed-Word2Vec-BERT_20221121011706.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(SAVED_MODEL)\n",
    "\n",
    "training_args = TrainingArguments(output_dir=SAVED_MODEL,report_to=\"none\")\n",
    "trainer = Trainer(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 70000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a58ae319b8411382714e120c31930b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = trainer.predict(small_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Com.      0.745     0.455     0.565      5000\n",
      "        Edu.      0.663     0.660     0.662      5000\n",
      "        Art.      0.861     0.577     0.691      5000\n",
      "        Ath.      0.820     0.940     0.876      5000\n",
      "        Off.      0.754     0.651     0.699      5000\n",
      "        Mea.      0.435     0.469     0.451      5000\n",
      "        Bui.      0.775     0.373     0.503      5000\n",
      "        Nat.      0.280     0.576     0.377      5000\n",
      "        Vil.      0.632     0.978     0.768      5000\n",
      "        Ani.      0.651     0.368     0.470      5000\n",
      "        Pla.      0.608     0.630     0.619      5000\n",
      "        Alb.      0.910     0.880     0.895      5000\n",
      "        Fil.      0.801     0.547     0.650      5000\n",
      "        Wri.      0.546     0.711     0.618      5000\n",
      "\n",
      "    accuracy                          0.630     70000\n",
      "   macro avg      0.677     0.630     0.632     70000\n",
      "weighted avg      0.677     0.630     0.632     70000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_pred = [np.argmax(i) for i in pred.predictions]\n",
    "\n",
    "target_names = [c[:3]+\".\" for c in classes]\n",
    "\n",
    "rep = classification_report(pred.label_ids, y_pred, target_names=target_names, digits=3)\n",
    "print(rep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('3.9.1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ad87dd819912217535c9e765bf7b9ebff52cfacfae1ecabbf23377bfd4399d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
