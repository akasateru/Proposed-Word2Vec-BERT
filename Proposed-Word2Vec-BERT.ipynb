{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dbl/.pyenv/versions/3.6.9/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification, logging\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import gensim.downloader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import string\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "\n",
    "np.random.seed(0)\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "MODEL = \"bert-base-uncased\"\n",
    "THRESHOLD = 0.05\n",
    "MAXLEN_GET_PSEUDO = 3000\n",
    "EPOCH = 10\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理\n",
    "def preprocessing(text):\n",
    "    # 括弧内文章の削除\n",
    "    text = re.sub(r'\\(.*\\)',' ',text)\n",
    "    text = re.sub(r'\\[.*\\]',' ',text)\n",
    "    text = re.sub(r'\\<.*\\>',' ',text)\n",
    "    text = re.sub(r'\\{.*\\}',' ',text)\n",
    "    # 記号文字の削除\n",
    "    text = text.translate(str.maketrans('','',string.punctuation))\n",
    "    # スペースの調整\n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18846/18846 [00:01<00:00, 17927.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# 20 newsgroups datasets\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups = fetch_20newsgroups(subset=\"all\")\n",
    "newsgroups_datasets = list()\n",
    "\n",
    "# # example ------------------------------------------------\n",
    "# for texts in tqdm(newsgroups.data[:10000]):\n",
    "#   texts = texts.split(\"\\n\\n\")\n",
    "#   texts = \" \".join(texts[1:])\n",
    "#   newsgroups_datasets.append(preprocessing(texts))\n",
    "# # --------------------------------------------------------\n",
    "\n",
    "for texts in tqdm(newsgroups.data):\n",
    "  texts = texts.split(\"\\n\\n\")\n",
    "  texts = \" \".join(texts[1:])\n",
    "  newsgroups_datasets.append(preprocessing(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1300000/1300000 [00:27<00:00, 47258.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# yahoo topic datasets\n",
    "with open('../data/topic/train_pu_half_v0.txt','r',encoding='utf-8') as f:\n",
    "    texts_v0 = f.read()\n",
    "with open('../data/topic/train_pu_half_v1.txt','r',encoding='utf-8') as f:\n",
    "    texts_v1 = f.read()\n",
    "texts = texts_v0 + texts_v1\n",
    "topic_datasets = list()\n",
    "\n",
    "# # example ----------------------------------------------\n",
    "# for label_text in tqdm(texts.splitlines()[:10000]):\n",
    "#   _, text = label_text.split(\"\\t\")\n",
    "#   topic_datasets.append(preprocessing(text))\n",
    "# # -------------------------------------------------------\n",
    "\n",
    "for label_text in tqdm(texts.splitlines()):\n",
    "  _, text = label_text.split(\"\\t\")\n",
    "  topic_datasets.append(preprocessing(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 762027/762027 [00:30<00:00, 25352.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# reuters datasets\n",
    "with open(\"../data/reuter/sourceall.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "  reuter = f.read().split(\"\\n\")[:-1]\n",
    "\n",
    "# # example -----------------------------------\n",
    "# reuter = reuter[:10000]\n",
    "# # -------------------------------------------\n",
    "\n",
    "reuters_datasets = list()\n",
    "for label_text in tqdm(reuter):\n",
    "  _, text = label_text.split(\"\\t\")\n",
    "  reuters_datasets.append(preprocessing(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 560000/560000 [00:08<00:00, 67661.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# dbpedia datasets train\n",
    "with open('../data/dbpedia_csv/train.csv','r',encoding='utf-8') as f:\n",
    "    reader = [r for r in csv.reader(f)]\n",
    "    \n",
    "# # example -------------------\n",
    "# reader = reader[:10000]\n",
    "# #----------------------------\n",
    "\n",
    "dbpedia_train_datasets = list()\n",
    "for _, auth, text in tqdm(reader):\n",
    "    text = text.replace(auth,'')\n",
    "    dbpedia_train_datasets.append(preprocessing(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbpedia classes\n",
    "with open(\"../data/dbpedia_csv/classes.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "  classes = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_texts = newsgroups_datasets + topic_datasets + reuters_datasets + dbpedia_train_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = gensim.downloader.load('word2vec-google-news-300')\n",
    "\n",
    "def w2v_avg_vector(sentence):\n",
    "  vector = np.zeros((300,), dtype=\"float32\")\n",
    "  count = 0\n",
    "  for word in sentence.split():\n",
    "    try:\n",
    "      vector = np.add(vector, word2vec[word])\n",
    "      count += 1\n",
    "    except:\n",
    "      pass\n",
    "  if count > 0:\n",
    "    vector = np.divide(vector, len(word))\n",
    "  return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_vector = list()\n",
    "for cls in classes:\n",
    "  classes_vector.append(w2v_avg_vector(cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2640873/2640873 [14:04<00:00, 3125.90it/s]\n"
     ]
    }
   ],
   "source": [
    "diff_datasets = {i:[] for i in range(len(classes))}\n",
    "for texts in tqdm(datasets_texts):\n",
    "  texts_vector = w2v_avg_vector(texts)\n",
    "  similarity = cosine_similarity([texts_vector], classes_vector)[0]\n",
    "  sim_argsorted = np.argsort(similarity)\n",
    "  diff = similarity[sim_argsorted[-1]] - similarity[sim_argsorted[-2]]\n",
    "  if diff > THRESHOLD:\n",
    "    diff_datasets[sim_argsorted[-1]].append((similarity[sim_argsorted[-1]], texts))\n",
    "\n",
    "pseudo_texts = list()\n",
    "pseudo_labels = list()\n",
    "for i in range(len(classes)):\n",
    "  sorted_diff_data = sorted(diff_datasets[i], reverse=True)[:MAXLEN_GET_PSEUDO]\n",
    "  pseudo_texts.extend([i[1] for i in sorted_diff_data])\n",
    "  pseudo_labels.extend([i]*len(sorted_diff_data[:MAXLEN_GET_PSEUDO]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all selected data\n",
      "Com. : 61181\n",
      "Edu. : 34672\n",
      "Art. : 6818\n",
      "Ath. : 16123\n",
      "Off. : 126859\n",
      "Mea. : 144368\n",
      "Bui. : 10120\n",
      "Nat. : 38562\n",
      "Vil. : 35431\n",
      "Ani. : 4742\n",
      "Pla. : 13933\n",
      "Alb. : 42379\n",
      "Fil. : 26335\n",
      "Wri. : 23263\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of all selected data\")\n",
    "for i in diff_datasets:\n",
    "  print(classes[i][:3]+\". : \"+str(len(diff_datasets[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dbl/.pyenv/versions/3.6.9/lib/python3.6/site-packages/keras/backend.py:4907: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  '\"`sparse_categorical_crossentropy` received `from_logits=True`, but '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5250/5250 [==============================] - 1173s 222ms/step - loss: 0.2030 - sparse_categorical_accuracy: 0.9413\n",
      "Epoch 2/10\n",
      "5250/5250 [==============================] - 1168s 223ms/step - loss: 0.0622 - sparse_categorical_accuracy: 0.9830\n",
      "Epoch 3/10\n",
      "5250/5250 [==============================] - 1170s 223ms/step - loss: 0.0380 - sparse_categorical_accuracy: 0.9898\n",
      "Epoch 4/10\n",
      "5250/5250 [==============================] - 1169s 223ms/step - loss: 0.0278 - sparse_categorical_accuracy: 0.9929\n",
      "Epoch 5/10\n",
      "5250/5250 [==============================] - 1169s 223ms/step - loss: 0.0212 - sparse_categorical_accuracy: 0.9946\n",
      "Epoch 6/10\n",
      "5250/5250 [==============================] - 1170s 223ms/step - loss: 0.0232 - sparse_categorical_accuracy: 0.9944\n",
      "Epoch 7/10\n",
      "5250/5250 [==============================] - 1168s 223ms/step - loss: 0.0161 - sparse_categorical_accuracy: 0.9964\n",
      "Epoch 8/10\n",
      "5250/5250 [==============================] - 1171s 223ms/step - loss: 0.0164 - sparse_categorical_accuracy: 0.9961\n",
      "Epoch 9/10\n",
      "5250/5250 [==============================] - 1170s 223ms/step - loss: 0.0127 - sparse_categorical_accuracy: 0.9972\n",
      "Epoch 10/10\n",
      "5250/5250 [==============================] - 1169s 223ms/step - loss: 0.0131 - sparse_categorical_accuracy: 0.9970\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f77d8047c18>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "x_train = tokenizer(pseudo_texts, truncation=True, return_tensors=\"tf\", padding=\"max_length\", max_length=512)\n",
    "y_train = np.array(pseudo_labels)\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "model.classifier = tf.keras.layers.Dense(units=14, activation=\"softmax\", name=\"classifer\")\n",
    "model.compile(optimizer=keras.optimizers.Adam(3e-5), \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=tf.metrics.SparseCategoricalAccuracy())\n",
    "model.fit(x_train[\"input_ids\"], y_train, batch_size=BATCH_SIZE, epochs=EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Com.      1.000     1.000     1.000      3000\n",
      "        Edu.      1.000     1.000     1.000      3000\n",
      "        Art.      0.998     1.000     0.999      3000\n",
      "        Ath.      1.000     0.999     0.999      3000\n",
      "        Off.      0.998     1.000     0.999      3000\n",
      "        Mea.      1.000     0.999     1.000      3000\n",
      "        Bui.      1.000     1.000     1.000      3000\n",
      "        Nat.      0.998     0.999     0.999      3000\n",
      "        Vil.      1.000     0.996     0.998      3000\n",
      "        Ani.      0.999     0.999     0.999      3000\n",
      "        Pla.      1.000     1.000     1.000      3000\n",
      "        Alb.      1.000     1.000     1.000      3000\n",
      "        Fil.      0.999     1.000     1.000      3000\n",
      "        Wri.      1.000     1.000     1.000      3000\n",
      "\n",
      "    accuracy                          0.999     42000\n",
      "   macro avg      0.999     0.999     0.999     42000\n",
      "weighted avg      0.999     0.999     0.999     42000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(x_train[\"input_ids\"], batch_size=BATCH_SIZE)\n",
    "y_pred = [np.argmax(i) for i in pred.logits]\n",
    "\n",
    "target_names = [c[:3]+\".\" for c in classes]\n",
    "rep = classification_report(y_train, y_pred, target_names=target_names, digits=3)\n",
    "print(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:01<00:00, 64756.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# load test data\n",
    "# dbpedia datasets train\n",
    "with open('../data/dbpedia_csv/test.csv','r',encoding='utf-8') as f:\n",
    "    reader = [r for r in csv.reader(f)]\n",
    "    \n",
    "# # example -------------------\n",
    "# import random\n",
    "# reader = random.sample(reader, 1000)\n",
    "# #----------------------------\n",
    "\n",
    "test_texts = list()\n",
    "test_labels = list()\n",
    "for labels, auth, text in tqdm(reader):\n",
    "    text = text.replace(auth,'')\n",
    "    test_texts.append(preprocessing(text))\n",
    "    test_labels.append(int(labels)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = tokenizer(test_texts, truncation=True, return_tensors=\"tf\", padding=\"max_length\", max_length=512)\n",
    "y_test = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Com.      0.789     0.497     0.610      5000\n",
      "        Edu.      0.678     0.679     0.678      5000\n",
      "        Art.      0.750     0.619     0.678      5000\n",
      "        Ath.      0.934     0.917     0.925      5000\n",
      "        Off.      0.687     0.663     0.675      5000\n",
      "        Mea.      0.453     0.525     0.486      5000\n",
      "        Bui.      0.857     0.377     0.524      5000\n",
      "        Nat.      0.309     0.707     0.430      5000\n",
      "        Vil.      0.763     0.954     0.848      5000\n",
      "        Ani.      0.637     0.658     0.647      5000\n",
      "        Pla.      0.826     0.394     0.533      5000\n",
      "        Alb.      0.926     0.851     0.887      5000\n",
      "        Fil.      0.827     0.542     0.655      5000\n",
      "        Wri.      0.541     0.743     0.626      5000\n",
      "\n",
      "    accuracy                          0.652     70000\n",
      "   macro avg      0.713     0.652     0.657     70000\n",
      "weighted avg      0.713     0.652     0.657     70000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(x_test[\"input_ids\"], batch_size=BATCH_SIZE)\n",
    "y_pred = [np.argmax(i) for i in pred.logits]\n",
    "target_names = [c[:3]+\".\" for c in classes]\n",
    "rep = classification_report(y_test, y_pred, target_names=target_names, digits=3)\n",
    "print(rep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('3.6.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b11d1de90a7344cdfbee299251a47ba7fb912949f3086a91bbe84e05957082a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
