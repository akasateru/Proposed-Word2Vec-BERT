{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Word2Vec-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForSequenceClassification, logging\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gensim.downloader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import datasets\n",
    "import string\n",
    "import evaluate\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "logging.set_verbosity_error()\n",
    "logging.set_verbosity_warning()\n",
    "HF_HUB_DISABLE_SYMLINKS_WARNING = True\n",
    "\n",
    "import datetime\n",
    "t_delta = datetime.timedelta(hours=9)\n",
    "JST = datetime.timezone(t_delta, 'JST')\n",
    "now = datetime.datetime.now(JST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "MODEL = \"bert-base-uncased\"\n",
    "SAVED_MODEL = \"../model/Proposed-Word2Vec-BERT_\"+str(now.strftime('%Y%m%d%H%M%S'))\n",
    "THRESHOLD = 0.05\n",
    "MAXLEN_GET_PSEUDO = 3000\n",
    "MAX_LEN = 128\n",
    "EPOCH = 5\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../model/Proposed-Word2Vec-BERT_20221202133539\n"
     ]
    }
   ],
   "source": [
    "print(SAVED_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理\n",
    "def preprocessing(text):\n",
    "    # 括弧内文章の削除\n",
    "    text = re.sub(r'\\(.*\\)',' ',text)\n",
    "    text = re.sub(r'\\[.*\\]',' ',text)\n",
    "    text = re.sub(r'\\<.*\\>',' ',text)\n",
    "    text = re.sub(r'\\{.*\\}',' ',text)\n",
    "    # 記号文字の削除\n",
    "    text = text.translate(str.maketrans('','',string.punctuation))\n",
    "    # スペースの調整\n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18846/18846 [00:01<00:00, 17426.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# 20 newsgroups datasets\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups = fetch_20newsgroups(subset=\"all\")\n",
    "newsgroups_datasets = list()\n",
    "\n",
    "# # example ------------------------------------------------\n",
    "# for texts in tqdm(newsgroups.data[:100]):\n",
    "#   texts = texts.split(\"\\n\\n\")\n",
    "#   texts = \" \".join(texts[1:])\n",
    "#   newsgroups_datasets.append(preprocessing(texts))\n",
    "# # --------------------------------------------------------\n",
    "\n",
    "for texts in tqdm(newsgroups.data):\n",
    "  texts = texts.split(\"\\n\\n\")\n",
    "  texts = \" \".join(texts[1:])\n",
    "  newsgroups_datasets.append(preprocessing(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1300000/1300000 [00:29<00:00, 44809.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# yahoo topic datasets\n",
    "with open('../data/topic/train_pu_half_v0.txt','r',encoding='utf-8') as f:\n",
    "    texts_v0 = f.read()\n",
    "with open('../data/topic/train_pu_half_v1.txt','r',encoding='utf-8') as f:\n",
    "    texts_v1 = f.read()\n",
    "texts = texts_v0 + texts_v1\n",
    "topic_datasets = list()\n",
    "\n",
    "# # example ----------------------------------------------\n",
    "# for label_text in tqdm(texts.splitlines()[:100]):\n",
    "#   _, text = label_text.split(\"\\t\")\n",
    "#   topic_datasets.append(preprocessing(text))\n",
    "# # -------------------------------------------------------\n",
    "\n",
    "for label_text in tqdm(texts.splitlines()):\n",
    "  _, text = label_text.split(\"\\t\")\n",
    "  topic_datasets.append(preprocessing(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 762027/762027 [00:30<00:00, 24932.55it/s]\n"
     ]
    }
   ],
   "source": [
    "# reuters datasets\n",
    "with open(\"../data/reuter/sourceall.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "  reuter = f.read().split(\"\\n\")[:-1]\n",
    "\n",
    "# # example -------------------\n",
    "# import random\n",
    "# reuter = random.sample(reuter, 100)\n",
    "# #----------------------------\n",
    "\n",
    "reuters_datasets = list()\n",
    "for label_text in tqdm(reuter):\n",
    "  _, text = label_text.split(\"\\t\")\n",
    "  reuters_datasets.append(preprocessing(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 560000/560000 [00:08<00:00, 64327.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# dbpedia datasets train\n",
    "with open('../data/dbpedia_csv/train.csv','r',encoding='utf-8') as f:\n",
    "    reader = [r for r in csv.reader(f)]\n",
    "    \n",
    "# # example -------------------\n",
    "# import random\n",
    "# reader = random.sample(reader, 100)\n",
    "# #----------------------------\n",
    "\n",
    "dbpedia_train_datasets = list()\n",
    "for _, auth, text in tqdm(reader):\n",
    "    text = text.replace(auth,'')\n",
    "    dbpedia_train_datasets.append(preprocessing(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbpedia classes\n",
    "with open(\"../data/dbpedia_csv/classes.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "  classes = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_texts = newsgroups_datasets + topic_datasets + reuters_datasets + dbpedia_train_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choice method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文章をベクトルに変換\n",
    "# 文章内に複数同じ単語が出現する場合、1度だけ使用する\n",
    "def w2v_avg_vector(sentence):\n",
    "  vector = np.zeros((300,), dtype=\"float32\")\n",
    "  count = 0\n",
    "  used_word = list()\n",
    "  for word in sentence.split():\n",
    "    if word not in used_word:\n",
    "      used_word.append(word)\n",
    "      try:\n",
    "        vector = np.add(vector, word2vec[word])\n",
    "        count += 1\n",
    "      except:\n",
    "        pass\n",
    "    if count >= MAX_LEN:\n",
    "      break\n",
    "  if count > 0:\n",
    "    vector = np.divide(vector, count)\n",
    "  return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_vector = [w2v_avg_vector(cls) for cls in classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2640873/2640873 [11:24<00:00, 3859.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# 情報源領域の文章と各クラスの類似度を計算し、上位2クラスの差が閾値を超えた場合、1位のクラスの学習データとする\n",
    "diff_datasets = {i:[] for i in range(len(classes))}\n",
    "for texts in tqdm(datasets_texts):\n",
    "  texts_vector = w2v_avg_vector(texts)\n",
    "  similarity = cosine_similarity([texts_vector], classes_vector)[0]\n",
    "  sim_argsorted = np.argsort(similarity)\n",
    "  diff = similarity[sim_argsorted[-1]] - similarity[sim_argsorted[-2]]\n",
    "  if diff >= THRESHOLD:\n",
    "    diff_datasets[sim_argsorted[-1]].append((diff, texts))\n",
    "\n",
    "# 上位2クラスの差が閾値を超えた文章の中で、その差が大きいものから順に各クラスの疑似ラベル付きデータの数が同じになるように選択\n",
    "pseudo_texts = list()\n",
    "pseudo_labels = list()\n",
    "for i in range(len(classes)):\n",
    "  sorted_diff_data = sorted(diff_datasets[i], reverse=True)[:MAXLEN_GET_PSEUDO]\n",
    "  pseudo_texts.extend([i[1] for i in sorted_diff_data])\n",
    "  pseudo_labels.extend([i]*len(sorted_diff_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all selected data\n",
      "Com. : 71598\n",
      "Edu. : 36764\n",
      "Art. : 7625\n",
      "Ath. : 17758\n",
      "Off. : 86410\n",
      "Mea. : 204570\n",
      "Bui. : 9172\n",
      "Nat. : 37282\n",
      "Vil. : 34984\n",
      "Ani. : 4312\n",
      "Pla. : 13165\n",
      "Alb. : 39291\n",
      "Fil. : 23275\n",
      "Wri. : 23976\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of all selected data\")\n",
    "for i in diff_datasets:\n",
    "  print(classes[i][:3]+\". : \"+str(len(diff_datasets[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70000/70000 [00:01<00:00, 62255.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# load test data\n",
    "# dbpedia datasets train\n",
    "with open('../data/dbpedia_csv/test.csv','r',encoding='utf-8') as f:\n",
    "    reader = [r for r in csv.reader(f)]\n",
    "    \n",
    "# # example -------------------\n",
    "# import random\n",
    "# reader = random.sample(reader, 100)\n",
    "# #----------------------------\n",
    "\n",
    "test_texts = list()\n",
    "test_labels = list()\n",
    "for labels, auth, text in tqdm(reader):\n",
    "    text = text.replace(auth,'')\n",
    "    test_texts.append(preprocessing(text))\n",
    "    test_labels.append(int(labels)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad0bb7e4a7d42e89d7bc51b9f765aa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db6d9f5f36814542998eb96b13e0a67d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 42000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 70000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "train_dataset = datasets.Dataset.from_dict({\"text\":pseudo_texts, \"label\":pseudo_labels})\n",
    "test_dataset = datasets.Dataset.from_dict({\"text\":test_texts, \"label\":test_labels})\n",
    "dataset = datasets.DatasetDict({\"train\":train_dataset, \"test\":test_dataset})\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, return_tensors=\"pt\", padding=\"max_length\", max_length=512)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns('text')\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42) #.select(range(5000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42) #.select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return evaluate.load(\"accuracy\").compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "  output_dir=SAVED_MODEL,\n",
    "  num_train_epochs=EPOCH,\n",
    "  per_device_train_batch_size=BATCH_SIZE,\n",
    "  per_device_eval_batch_size=BATCH_SIZE,\n",
    "  evaluation_strategy=\"epoch\",\n",
    "  logging_strategy='epoch',\n",
    "  save_strategy=\"no\",\n",
    "  optim=\"adamw_torch\",\n",
    "  report_to=\"none\",\n",
    "  )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 42000\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6565\n",
      "  Number of trainable parameters = 109493006\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f4b2ab2cfca4e53b7e1ed1d3bddd3c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6565 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 70000\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1337, 'learning_rate': 4e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e9e4c146494a869d644c22c3c1bce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.098001003265381, 'eval_accuracy': 0.6683, 'eval_runtime': 541.6741, 'eval_samples_per_second': 129.229, 'eval_steps_per_second': 4.039, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 70000\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0199, 'learning_rate': 3e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d72996def694c6c80299fe48bdd8490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.270796060562134, 'eval_accuracy': 0.6783285714285714, 'eval_runtime': 541.6432, 'eval_samples_per_second': 129.236, 'eval_steps_per_second': 4.04, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 70000\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0064, 'learning_rate': 2e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41dd95619c3842558233208ad5900700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.805812120437622, 'eval_accuracy': 0.6566428571428572, 'eval_runtime': 541.5976, 'eval_samples_per_second': 129.247, 'eval_steps_per_second': 4.04, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 70000\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.002, 'learning_rate': 1e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c8953421479433db6ddc33a50595ae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.571115493774414, 'eval_accuracy': 0.6849428571428572, 'eval_runtime': 542.153, 'eval_samples_per_second': 129.115, 'eval_steps_per_second': 4.036, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 70000\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0005, 'learning_rate': 0.0, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d66c2fbd01c04463806b5995175c6be9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.8686935901641846, 'eval_accuracy': 0.6702142857142858, 'eval_runtime': 542.5422, 'eval_samples_per_second': 129.022, 'eval_steps_per_second': 4.033, 'epoch': 5.0}\n",
      "{'train_runtime': 7231.1502, 'train_samples_per_second': 29.041, 'train_steps_per_second': 0.908, 'train_loss': 0.032508407379958075, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6565, training_loss=0.032508407379958075, metrics={'train_runtime': 7231.1502, 'train_samples_per_second': 29.041, 'train_steps_per_second': 0.908, 'train_loss': 0.032508407379958075, 'epoch': 5.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../model/Proposed-Word2Vec-BERT_20221202133539/config.json\n",
      "Model weights saved in ../model/Proposed-Word2Vec-BERT_20221202133539/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(SAVED_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../model/Proposed-Word2Vec-BERT_20221202133539/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"../model/Proposed-Word2Vec-BERT_20221202133539\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file ../model/Proposed-Word2Vec-BERT_20221202133539/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ../model/Proposed-Word2Vec-BERT_20221202133539.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(SAVED_MODEL)\n",
    "\n",
    "training_args = TrainingArguments(output_dir=SAVED_MODEL,report_to=\"none\")\n",
    "trainer = Trainer(model=model, args=training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 70000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b8d42f5cf741adbc70bb62c74dbc65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = trainer.predict(small_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Com.      0.786     0.499     0.610      5000\n",
      "        Edu.      0.716     0.863     0.783      5000\n",
      "        Art.      0.782     0.538     0.638      5000\n",
      "        Ath.      0.935     0.790     0.856      5000\n",
      "        Off.      0.723     0.634     0.675      5000\n",
      "        Mea.      0.683     0.558     0.615      5000\n",
      "        Bui.      0.640     0.557     0.596      5000\n",
      "        Nat.      0.306     0.738     0.433      5000\n",
      "        Vil.      0.745     0.978     0.846      5000\n",
      "        Ani.      0.795     0.320     0.457      5000\n",
      "        Pla.      0.783     0.423     0.549      5000\n",
      "        Alb.      0.869     0.967     0.916      5000\n",
      "        Fil.      0.868     0.705     0.778      5000\n",
      "        Wri.      0.553     0.812     0.658      5000\n",
      "\n",
      "    accuracy                          0.670     70000\n",
      "   macro avg      0.727     0.670     0.672     70000\n",
      "weighted avg      0.727     0.670     0.672     70000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_pred = [np.argmax(i) for i in pred.predictions]\n",
    "target_names = [c[:3]+\".\" for c in classes]\n",
    "rep = classification_report(pred.label_ids, y_pred, target_names=target_names, digits=3)\n",
    "print(rep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit ('3.10.4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49f61df75a858f28f3874af47299bfac7972d99f8ad49edf3a81b2f4a8173a97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
